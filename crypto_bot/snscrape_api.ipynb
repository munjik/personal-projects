{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1064c38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa92c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('$BTC').get_items()):\n",
    "#     if i > 300:\n",
    "#         break\n",
    "#     tweets_list.append([tweet.date, tweet.content])\n",
    "# # # Creating a dataframe from the tweets list above\n",
    "# tweets_df = pd.DataFrame(tweets_list, columns=['created_at', 'full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7c3d787",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Downloading yfinance-0.1.64.tar.gz (26 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.24 in /Users/munjikahalah/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from yfinance) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /Users/munjikahalah/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from yfinance) (1.19.5)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/munjikahalah/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from yfinance) (2.25.1)\n",
      "Collecting multitasking>=0.0.7\n",
      "  Downloading multitasking-0.0.9.tar.gz (8.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.5.1 in /Users/munjikahalah/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from yfinance) (4.6.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/munjikahalah/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas>=0.24->yfinance) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/munjikahalah/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/munjikahalah/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from requests>=2.20->yfinance) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/munjikahalah/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from requests>=2.20->yfinance) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/munjikahalah/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from requests>=2.20->yfinance) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/munjikahalah/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from requests>=2.20->yfinance) (1.26.6)\n",
      "Requirement already satisfied: six>=1.5 in /Users/munjikahalah/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
      "Building wheels for collected packages: yfinance, multitasking\n",
      "  Building wheel for yfinance (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for yfinance: filename=yfinance-0.1.64-py2.py3-none-any.whl size=24095 sha256=f8e354513abb3dbe0e61637bc767f075e7aff6e79f1c45dfbc6bc1d65c5c1048\n",
      "  Stored in directory: /Users/munjikahalah/Library/Caches/pip/wheels/f9/e9/7e/8b13db3bf3aeb5049d759e10702736fb96753089ac950fddc0\n",
      "  Building wheel for multitasking (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for multitasking: filename=multitasking-0.0.9-py3-none-any.whl size=8368 sha256=c4b45a3c13eb8c959a84407fedee5e90c4180c012c7f831be9fcf6cff4d71983\n",
      "  Stored in directory: /Users/munjikahalah/Library/Caches/pip/wheels/57/6d/a3/a39b839cc75274d2acfb1c58bfead2f726c6577fe8c4723f13\n",
      "Successfully built yfinance multitasking\n",
      "Installing collected packages: multitasking, yfinance\n",
      "Successfully installed multitasking-0.0.9 yfinance-0.1.64\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1784499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "import yfinance as yf\n",
    "# import tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c84f29a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacen(x):\n",
    "    return x.replace('\\n','')\n",
    "def replacer(x):\n",
    "    return x.replace('\\r','')\n",
    "def replacern(x):\n",
    "    return x.replace('\\r\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0da1300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df['full_text'] = df['full_text'].apply(replacer)\n",
    "    df['full_text'] = df['full_text'].apply(replacen)\n",
    "    df['full_text'] = df['full_text'].apply(replacern)\n",
    "    return df\n",
    "def remove_dollar(l):\n",
    "    new_list = []\n",
    "    for item in l:\n",
    "        new_list.append(item.replace('$',''))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66446e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    return text\n",
    "def remove_url(df):\n",
    "    df['full_text'] = df['full_text'].apply(remove_urls)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63d323ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tickers(text):\n",
    "    return re.findall(r'[$][A-Za-z][\\S]*', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c36d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "duplicated_tweet = []\n",
    "def get_tweets(tickers):\n",
    "    tweets_list = []\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper('${}'.format(tickers)).get_items()):\n",
    "        if i > 1000:\n",
    "            break\n",
    "        tweets_list.append([tweet.date, tweet.content])\n",
    "        tweet_df = pd.DataFrame(tweets_list, columns=['created_at', 'full_text'])\n",
    "        duplicated_tweet.append( tweet_df.sum().duplicated())\n",
    "        tweet_df['symbols'] = tweet_df['full_text'].apply(find_tickers)\n",
    "        tweet_df['symbols'] = tweet_df['symbols'].apply(remove_dollar)\n",
    "        download_yahoo_stocks(tickers= tickers, period='6mo')\n",
    "        \n",
    "    return tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ad5abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_tweets('SOL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0b456d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fw/j6mc6mcd20379_z864ff2bfw0000gn/T/ipykernel_15395/3011636862.py:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  duplicated_tweet.append( tweet_df.sum().duplicated())\n"
     ]
    }
   ],
   "source": [
    "dataframe = get_tweets('DOGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08d6c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = clean_data(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09282ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_matrix(df):\n",
    "    embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
    "    text_result = embed(df['full_text'])\n",
    "    vector_df = pd.DataFrame(text_result.numpy())\n",
    "    return vector_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68866a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = remove_url(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a2adb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@BadLuckBanana1 This is not about NFTs, i want to talk about a #Crypto i think is the next big one, if you're not interested remember that you were not interested in $Bit $Eth $Doge $shib years ago and ask to your friends who know about #altcoin to check this out $Tking\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.full_text[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f91c732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_datetime(df):\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e74051b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_tweets_per_day(df):\n",
    "    ticker_mean_df = df.groupby('created_at').mean().reset_index()\n",
    "    print(f'TICKER MEAN DF SHAPE =========== {ticker_mean_df.shape}')\n",
    "    return ticker_mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "661ee5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_yahoo_stocks(tickers=tickers, period = \"6mo\"):\n",
    "#     tickers = yf.download(tickers = tickers, interval='1d',period = period)\n",
    "\n",
    "#     INTCclose = tickers['Close']['INTC']\n",
    "#     BYNDclose = tickers['Close']['BYND']\n",
    "#     GEclose = tickers['Close']['GE']\n",
    "#     BTCclose = tickers['Close']['BTC']\n",
    "\n",
    "#     # return {\"BTC\": BTCclose, \"BYND\": BYNDclose,\n",
    "\n",
    "#     return INTCclose, BYNDclose, GEclose, BTCclose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd8d1a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_date_time(df):\n",
    "    if df['created_at'].dtype == 'object':\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    df['created_at'] = df['created_at'].dt.date\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97f4e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_live_tweets_to_vectorize(dataframe):\n",
    "    df = dataframe\n",
    "    df = fix_date_time(df)\n",
    "#     df = concat_vectors(df)\n",
    "    df = embed_matrix(df)\n",
    "#     df = df_to_datetime(df)\n",
    "#     df = average_tweets_per_day(df)\n",
    "#     df = df.set_index(['created_at'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6ab47ac4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Op type not registered 'SentencepieceOp' in binary running on Munjis-MacBook-Pro.local. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n If trying to load on a different device from the computational device, consider using setting the `experimental_io_device` option on tf.saved_model.LoadOptions to the io_device such as '/job:localhost'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_op_def\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m   3957\u001b[0m     \u001b[0mcreate\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwish\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3958\u001b[0;31m     \u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myou\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mexplicitly\u001b[0m \u001b[0madd\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mwith\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3959\u001b[0m     \u001b[0mthread\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SentencepieceOp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    904\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m       raise ValueError(\"SavedModels saved from Tensorflow V1 or Estimator (any \"\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, filters)\u001b[0m\n\u001b[1;32m    132\u001b[0m             meta_graph.graph_def.library))\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/saved_model/function_deserialization.py\u001b[0m in \u001b[0;36mload_function_def_library\u001b[0;34m(library, load_shared_name_suffix)\u001b[0m\n\u001b[1;32m    357\u001b[0m       \u001b[0;31m# TODO(b/150708051): Remove this hack once TensorRT SavedModel integration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m       \u001b[0;31m# is fixed. Currently it's leaking memory to maintain bug compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m       \u001b[0;31m# with previous behavior.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/framework/function_def_to_graph.py\u001b[0m in \u001b[0;36mfunction_def_to_graph\u001b[0;34m(fdef, input_shapes)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Add all function nodes to the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mimporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def_for_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/framework/function_def_to_graph.py\u001b[0m in \u001b[0;36mfunction_def_to_graph_def\u001b[0;34m(fdef, input_shapes)\u001b[0m\n\u001b[1;32m    227\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"list(func)\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m           \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_op_def\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m   3961\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mfollowing\u001b[0m \u001b[0mcode\u001b[0m \u001b[0mexamples\u001b[0m \u001b[0mare\u001b[0m \u001b[0mequivalent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3962\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3963\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Op type not registered 'SentencepieceOp' in binary running on Munjis-MacBook-Pro.local. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fw/j6mc6mcd20379_z864ff2bfw0000gn/T/ipykernel_15395/3587650125.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mour_live_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrab_live_tweets_to_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fw/j6mc6mcd20379_z864ff2bfw0000gn/T/ipykernel_15395/3359426915.py\u001b[0m in \u001b[0;36mgrab_live_tweets_to_vectorize\u001b[0;34m(dataframe)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix_date_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     df = concat_vectors(df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#     df = df_to_datetime(df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     df = average_tweets_per_day(df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fw/j6mc6mcd20379_z864ff2bfw0000gn/T/ipykernel_15395/2196288684.py\u001b[0m in \u001b[0;36membed_matrix\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0membed_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtext_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvector_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvector_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m    104\u001b[0m         module_path, tags=tags, options=options)\n\u001b[1;32m    105\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m   \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;31m# Supports e.g. tags=SERVING and tags=[SERVING]. Sets aren't considered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;31m# sequences for nest.flatten, so we put those through as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m   saved_model_proto, debug_info = (\n\u001b[1;32m    871\u001b[0m       loader_impl.parse_saved_model_with_debug_info(export_dir))\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    906\u001b[0m       raise ValueError(\"SavedModels saved from Tensorflow V1 or Estimator (any \"\n\u001b[1;32m    907\u001b[0m                        \"version) cannot be loaded with node filters.\")\n\u001b[0;32m--> 908\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m       \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_v1_in_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_debug_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Op type not registered 'SentencepieceOp' in binary running on Munjis-MacBook-Pro.local. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n If trying to load on a different device from the computational device, consider using setting the `experimental_io_device` option on tf.saved_model.LoadOptions to the io_device such as '/job:localhost'."
     ]
    }
   ],
   "source": [
    "our_live_vector = grab_live_tweets_to_vectorize(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f1606e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>symbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-03</td>\n",
       "      <td>This trader fixed $100thousands on Ethereum sh...</td>\n",
       "      <td>[eth, omg, ldx, lrc, cate, mana, avax, hot, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-11-03</td>\n",
       "      <td>@SquidGamePRO @CoinMarketCap @business @nypost...</td>\n",
       "      <td>[SQUID, Floki, Shiba, Doge, SQUID, Floki, Shib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-11-03</td>\n",
       "      <td>@himacoin I'm so excited and happy to be parti...</td>\n",
       "      <td>[HIMA, SAMO, SOL, DOGE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-11-03</td>\n",
       "      <td>BugÃ¼n patlÄ±ycak hissediyorum $DOGE #doge sende...</td>\n",
       "      <td>[DOGE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-11-03</td>\n",
       "      <td>@himacoin This project have great potentials a...</td>\n",
       "      <td>[HIMA, SAMO, SOL, DOGE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>@SquidGamePRO @CoinMarketCap @business @nypost...</td>\n",
       "      <td>[SQUID, Floki, Shiba, Doge]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>$DOGE | $0.27447 | $0.27484 / $0.27281 / $0.27...</td>\n",
       "      <td>[DOGE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>SIIM, MEUS QUERIDOS!O @elonmusk tambÃ©m acha qu...</td>\n",
       "      <td>[DOGE., FLOKI]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>@SquidGamePRO @CoinMarketCap @business @nypost...</td>\n",
       "      <td>[SQUID, Floki, Shiba, Doge]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>ðŸš¨ Dogecoin Daily Update ðŸš¨ðŸ“ˆ 24HR change: 1.14% ...</td>\n",
       "      <td>[DOGE]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1001 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      created_at                                          full_text  \\\n",
       "0     2021-11-03  This trader fixed $100thousands on Ethereum sh...   \n",
       "1     2021-11-03  @SquidGamePRO @CoinMarketCap @business @nypost...   \n",
       "2     2021-11-03  @himacoin I'm so excited and happy to be parti...   \n",
       "3     2021-11-03  BugÃ¼n patlÄ±ycak hissediyorum $DOGE #doge sende...   \n",
       "4     2021-11-03  @himacoin This project have great potentials a...   \n",
       "...          ...                                                ...   \n",
       "996   2021-11-02  @SquidGamePRO @CoinMarketCap @business @nypost...   \n",
       "997   2021-11-02  $DOGE | $0.27447 | $0.27484 / $0.27281 / $0.27...   \n",
       "998   2021-11-02  SIIM, MEUS QUERIDOS!O @elonmusk tambÃ©m acha qu...   \n",
       "999   2021-11-02  @SquidGamePRO @CoinMarketCap @business @nypost...   \n",
       "1000  2021-11-02  ðŸš¨ Dogecoin Daily Update ðŸš¨ðŸ“ˆ 24HR change: 1.14% ...   \n",
       "\n",
       "                                                symbols  \n",
       "0     [eth, omg, ldx, lrc, cate, mana, avax, hot, co...  \n",
       "1     [SQUID, Floki, Shiba, Doge, SQUID, Floki, Shib...  \n",
       "2                               [HIMA, SAMO, SOL, DOGE]  \n",
       "3                                                [DOGE]  \n",
       "4                               [HIMA, SAMO, SOL, DOGE]  \n",
       "...                                                 ...  \n",
       "996                         [SQUID, Floki, Shiba, Doge]  \n",
       "997                                              [DOGE]  \n",
       "998                                      [DOGE., FLOKI]  \n",
       "999                         [SQUID, Floki, Shiba, Doge]  \n",
       "1000                                             [DOGE]  \n",
       "\n",
       "[1001 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_live_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c4a39f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_yahoo_stocks(tickers= 'SPY SPCE', period = \"6mo\"):\n",
    "    tickers = yf.download(tickers = tickers, interval='1d',period = period)\n",
    "\n",
    "    ticker_price = tickers['Close']['SPY']\n",
    "    nw = tickers['Close']['SPCE']\n",
    "\n",
    "    return ticker_pricem, nw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "881cdc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[                       0%                       ]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fw/j6mc6mcd20379_z864ff2bfw0000gn/T/ipykernel_15395/3945053274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstock_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_yahoo_stocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SPY SPCE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'6mo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fw/j6mc6mcd20379_z864ff2bfw0000gn/T/ipykernel_15395/548920611.py\u001b[0m in \u001b[0;36mdownload_yahoo_stocks\u001b[0;34m(tickers, period)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdownload_yahoo_stocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'SPY SPCE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"6mo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtickers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtickers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtickers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1d'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mperiod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mticker_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtickers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SPY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtickers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SPCE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/yfinance/multi.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(tickers, start, end, actions, threads, group_by, auto_adjust, back_adjust, progress, period, show_errors, interval, prepost, proxy, rounding, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m                                    rounding=rounding)\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DFS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0m_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# download synchronously\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stock_price = download_yahoo_stocks('SPY SPCE', '6mo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2905d12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2021-05-03    57.259998\n",
       "2021-05-04    56.900002\n",
       "2021-05-05    56.849998\n",
       "2021-05-06    57.189999\n",
       "2021-05-07    57.669998\n",
       "                ...    \n",
       "2021-10-27    47.889999\n",
       "2021-10-28    48.080002\n",
       "2021-10-29    49.000000\n",
       "2021-11-01    49.549999\n",
       "2021-11-02    49.860001\n",
       "Name: INTC, Length: 129, dtype: float64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INTCclose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30127e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
